{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_Class_Excercise_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreshtaP/-Shreshta_INFO5731_-Fall2021/blob/main/In_Class_Excercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDRroI84o7Dq"
      },
      "source": [
        "The third In-class-exercise (9/15/2021, 40 points in total)\n",
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis.\n",
        "\n",
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aDcoXfLpE6b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e29WQ-5g5zNP"
      },
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "Walmart reviews to know about customer satisfaction. \n",
        "The data that should be collected are the reviews that are given by the customers who shopped from Walmart and the customer reviews are taken from the website \n",
        "Huge amount of data is required and I have taken 1000 reviews.  \n",
        "\n",
        "Steps for collecting and saving the data: \n",
        "1) To extract the data from the website beautiful Soup is used.  \n",
        "2) By using the class name reviews are extracted and are appended to empty list \n",
        "3) A dataframe is created for the list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evkcS_s5pKyJ"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 reviews of a movie from IMDB (https://www.imdb.com/) or 1000 reviews of a product from Amazon (https://www.amazon.com/).\n",
        "\n",
        "As for the IMDB movie review, the following informtion need to be collected (for example: https://www.imdb.com/title/tt6751668/reviews?ref_=tt_urv):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time\n",
        "\n",
        "As for the Amazon product review, the following information need to be collected (for example: https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_3?crid=2E3C55VKJX0K3&dchild=1&keywords=machine+learning+andrew+ng&qid=1631718619&sr=8-3):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZdHkDJipT8v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACVoWdGF5zNR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "c0a3e16a-407b-414d-beef-bb454f7a0eb6"
      },
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import lxml.html as LH\n",
        "url = 'https://www.imdb.com/search/title/?title_type=feature'\n",
        "imdb_url = \"https://www.imdb.com\"\n",
        "response = requests.get(url)\n",
        "movies_soup = BeautifulSoup(response.text, 'lxml')\n",
        "movie_tags = movies_soup.find_all('a', attrs={'class': None})\n",
        "movie_tags = [tag.attrs['href'] for tag in movie_tags \n",
        "              if tag.attrs['href'].startswith('/title') & tag.attrs['href'].endswith('/')]\n",
        "movie_tags = list(dict.fromkeys(movie_tags))\n",
        "movie_links = [imdb_url + tag + 'reviews' for tag in movie_tags]\n",
        "def minMax(a):\n",
        "      print(len(a))\n",
        "      minpos = a.index(min(a))\n",
        "      maxpos = a.index(max(a))\n",
        "      return minpos, maxpos\n",
        "def getReviews(soup):\n",
        "    review_ratings = [tag.previous_element for tag in \n",
        "                           soup.find_all('span', attrs={'class': 'point-scale'})]\n",
        "    if len(review_ratings) > 0:\n",
        "      n_index, p_index = minMax(list(map(int, review_ratings)))\n",
        "      review_list = soup.find_all('a', attrs={'class':'title'})\n",
        "      reviews = [\"https://www.imdb.com\" + review['href'] for review in review_list]\n",
        "      return reviews\n",
        "    else:\n",
        "      return None\n",
        "def getSoup(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    return soup\n",
        "def getReviewText(review_url):\n",
        "    soup = getSoup(review_url)\n",
        "    tag = soup.find('div', attrs={'class': 'text show-more__control'})\n",
        "    return tag.getText()\n",
        "def getUserName(review_url):    \n",
        "    soup = getSoup(review_url)\n",
        "    tag = soup.find('h3')\n",
        "    return list(tag.children)[0].getText()\n",
        "def getMovieTitle(review_url):    \n",
        "    soup = getSoup(review_url)\n",
        "    tag = soup.find('h1')\n",
        "    return list(tag.children)[1].getText()\n",
        "def getReviewPostedDate(review_url):    \n",
        "    soup = getSoup(review_url)\n",
        "    tag = soup.find('div', attrs={'class': 'display-name-date'})\n",
        "    return list(tag.children)[1].getText()\n",
        "def getRatings(review_url):    \n",
        "    soup = getSoup(review_url)\n",
        "    tag = soup.find('div', attrs={'class': 'ipl-ratings-bar'})\n",
        "    return list(tag.children)[1].getText()\n",
        "movie_soups = getSoup(movie_links[0])\n",
        "movie_review = getReviews(movie_soups)\n",
        "imdb_review_texts = [getReviewText(url) for url in movie_review]\n",
        "user_Names = [getUserName(url) for url in movie_review]\n",
        "user_ratings = [getRatings(url) for url in movie_review]\n",
        "posted_dates = [getReviewPostedDate(url) for url in movie_review]\n",
        "data = {'User_Name': user_Names,\n",
        "        'Review_text': imdb_review_texts,\n",
        "        'Review_posted_time': posted_dates,\n",
        "        'Star': user_ratings}\n",
        "df = pd.DataFrame(data)\n",
        "df.head(25)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_Name</th>\n",
              "      <th>Review_text</th>\n",
              "      <th>Review_posted_time</th>\n",
              "      <th>Star</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dominicwood-14504</td>\n",
              "      <td>I'll start by saying that if you're looking fo...</td>\n",
              "      <td>4 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n9/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>laviniadallam</td>\n",
              "      <td>After 10 years of almost every movie being.arm...</td>\n",
              "      <td>2 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n10/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mhatreritesh</td>\n",
              "      <td>Perfect Fantasy film to watch with full family...</td>\n",
              "      <td>4 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n9/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nishantsalhotrans</td>\n",
              "      <td>Keeping it short. This movie had it all. Great...</td>\n",
              "      <td>6 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n10/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>truthtellers-16825</td>\n",
              "      <td>Brought to you by the Truth Tellers.Film is gr...</td>\n",
              "      <td>1 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n8/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>csheff3</td>\n",
              "      <td>Haven't been much of a Marvel guy even with th...</td>\n",
              "      <td>4 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n9/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>IliescuVictor</td>\n",
              "      <td>I had very few expectations from this one, giv...</td>\n",
              "      <td>2 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n9/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>handaloo</td>\n",
              "      <td>First off, this is a decent movie.Sure, there'...</td>\n",
              "      <td>4 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n7/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>delsoca</td>\n",
              "      <td>I have just watched this movie. I am an avid M...</td>\n",
              "      <td>4 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n7/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wgeddings</td>\n",
              "      <td>If you are like me, from the previews and ads ...</td>\n",
              "      <td>5 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n9/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>JATIN-1989</td>\n",
              "      <td>I watched this movie after Twitter was claimin...</td>\n",
              "      <td>4 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n4/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>MrSAsheC</td>\n",
              "      <td>It's a decent movie. Happy that the comedic ti...</td>\n",
              "      <td>2 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n5/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>adamthorn-31294</td>\n",
              "      <td>Shang-Chi was honestly amazing. The cinematic ...</td>\n",
              "      <td>5 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n10/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>gaberh1234</td>\n",
              "      <td>The fight scenes, creature designs and set are...</td>\n",
              "      <td>11 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n6/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Radio-1s_Mr-MovieMad-Ami_104-1FM</td>\n",
              "      <td>A -BIG- Screen Mini Review. Viewed Sept.05, 20...</td>\n",
              "      <td>8 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n9/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>MiroslavKyuranov</td>\n",
              "      <td>That was a freakin' blast! The story is comple...</td>\n",
              "      <td>1 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n8/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>z-00675</td>\n",
              "      <td>I could already guess the end watching only ha...</td>\n",
              "      <td>15 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n5/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>maximfilipenko</td>\n",
              "      <td>The film contains a lot of confusing plot poin...</td>\n",
              "      <td>2 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n5/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Izzy2727</td>\n",
              "      <td>Couple of things, 1. The actors are made of pl...</td>\n",
              "      <td>4 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n2/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>jameszd44</td>\n",
              "      <td>This movie is much better than black widow, it...</td>\n",
              "      <td>4 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n9/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>lakelazy</td>\n",
              "      <td>This movie portrays a villain that by far has ...</td>\n",
              "      <td>1 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n10/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>kinejin</td>\n",
              "      <td>Incredibly boring and indifferent film, with s...</td>\n",
              "      <td>19 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n5/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>teomslaer</td>\n",
              "      <td>Well the fight scenes were exciting , the stor...</td>\n",
              "      <td>3 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n4/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>valenkim</td>\n",
              "      <td>This is well done as far as special effects bu...</td>\n",
              "      <td>3 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n3/10\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>dzzg</td>\n",
              "      <td>Overall it's nothing new or very creative. I h...</td>\n",
              "      <td>3 September 2021</td>\n",
              "      <td>\\n\\n\\n\\n\\n\\n2/10\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           User_Name  ...                 Star\n",
              "0                  dominicwood-14504  ...   \\n\\n\\n\\n\\n\\n9/10\\n\n",
              "1                      laviniadallam  ...  \\n\\n\\n\\n\\n\\n10/10\\n\n",
              "2                       mhatreritesh  ...   \\n\\n\\n\\n\\n\\n9/10\\n\n",
              "3                  nishantsalhotrans  ...  \\n\\n\\n\\n\\n\\n10/10\\n\n",
              "4                 truthtellers-16825  ...   \\n\\n\\n\\n\\n\\n8/10\\n\n",
              "5                            csheff3  ...   \\n\\n\\n\\n\\n\\n9/10\\n\n",
              "6                      IliescuVictor  ...   \\n\\n\\n\\n\\n\\n9/10\\n\n",
              "7                           handaloo  ...   \\n\\n\\n\\n\\n\\n7/10\\n\n",
              "8                            delsoca  ...   \\n\\n\\n\\n\\n\\n7/10\\n\n",
              "9                          wgeddings  ...   \\n\\n\\n\\n\\n\\n9/10\\n\n",
              "10                        JATIN-1989  ...   \\n\\n\\n\\n\\n\\n4/10\\n\n",
              "11                          MrSAsheC  ...   \\n\\n\\n\\n\\n\\n5/10\\n\n",
              "12                   adamthorn-31294  ...  \\n\\n\\n\\n\\n\\n10/10\\n\n",
              "13                        gaberh1234  ...   \\n\\n\\n\\n\\n\\n6/10\\n\n",
              "14  Radio-1s_Mr-MovieMad-Ami_104-1FM  ...   \\n\\n\\n\\n\\n\\n9/10\\n\n",
              "15                  MiroslavKyuranov  ...   \\n\\n\\n\\n\\n\\n8/10\\n\n",
              "16                           z-00675  ...   \\n\\n\\n\\n\\n\\n5/10\\n\n",
              "17                    maximfilipenko  ...   \\n\\n\\n\\n\\n\\n5/10\\n\n",
              "18                          Izzy2727  ...   \\n\\n\\n\\n\\n\\n2/10\\n\n",
              "19                         jameszd44  ...   \\n\\n\\n\\n\\n\\n9/10\\n\n",
              "20                          lakelazy  ...  \\n\\n\\n\\n\\n\\n10/10\\n\n",
              "21                           kinejin  ...   \\n\\n\\n\\n\\n\\n5/10\\n\n",
              "22                         teomslaer  ...   \\n\\n\\n\\n\\n\\n4/10\\n\n",
              "23                          valenkim  ...   \\n\\n\\n\\n\\n\\n3/10\\n\n",
              "24                              dzzg  ...   \\n\\n\\n\\n\\n\\n2/10\\n\n",
              "\n",
              "[25 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvs21WWPpT5O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39g4Fxy15zNS"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/). \n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "L7oQU_jtotqr",
        "outputId": "647133d0-7feb-4224-a611-83f5e15b8b2f"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests, lxml, os, json\n",
        "import pandas as pd\n",
        "headers = {\n",
        "    'User-agent':\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
        "}\n",
        "params = {\n",
        "  \"q\": \"Information Science\",\n",
        "  \"hl\": \"en\",\n",
        "}\n",
        "google_link = requests.get('https://scholar.google.com/scholar', headers=headers, params = params).text\n",
        "soup = BeautifulSoup(google_link, 'lxml')\n",
        "for pdf in soup.select('.gs_or_ggsm a'):\n",
        "  pdf_file_link = pdf['href']\n",
        "  print(pdf_file_link)\n",
        "  list_of_data = []\n",
        "for result in soup.select('.gs_ri'):\n",
        "  title_header = result.select_one('.gs_rt').text\n",
        "  Information_publication = result.select_one('.gs_a').text\n",
        "  abstract = result.select_one('.gs_rs').text\n",
        "  try:\n",
        "    version_of_articles = result.select_one('a~ a+ .gs_nph')['href']\n",
        "  except:\n",
        "    version_of_articles = None\n",
        "  d = {\n",
        "      'Title': title_header,\n",
        "      'Authors/Venue/journal/conference being published/Year': Information_publication,\n",
        "      'Abstract': abstract\n",
        "  }\n",
        "  list_of_data.append(d)\n",
        "df = pd.DataFrame(list_of_data)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.marilia.unesp.br/Home/Instituicao/Docentes/EdbertoFerneda/k---artigo-01.pdf\n",
            "http://library.tuit.uz/knigiPDF/ang/5-606.pdf\n",
            "https://discovery.ucl.ac.uk/id/eprint/10053533/3/Longley_version%202.0_Manfred%20geocomp_PAL.pdf\n",
            "https://www.academia.edu/download/30771284/relevanceSaracevic.pdf\n",
            "http://www.garfield.library.upenn.edu/hwhite/whitejasist1998.pdf\n",
            "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.199.7788&rep=rep1&type=pdf\n",
            "http://eprints.rclis.org/6011/6/pt06.pdf\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Authors/Venue/journal/conference being published/Year</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Information science: what is it?</td>\n",
              "      <td>H Borko - American documentation, 1968 - Wiley...</td>\n",
              "      <td>In seeking a new sense of identity, we ask, in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The foundations of information science. Part I...</td>\n",
              "      <td>BC Brookes - Journal of information science, 1...</td>\n",
              "      <td>It is first argued that a niche for informatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[BOOK][B] Foundations of library and informati...</td>\n",
              "      <td>RE Rubin - 2017 - books.google.com</td>\n",
              "      <td>Much has happened since the last edition of th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Geographical information science</td>\n",
              "      <td>MF Goodchild - International journal of geogra...</td>\n",
              "      <td>Research papers at conferences such as EGIS an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[BOOK][B] Dictionary for library and informati...</td>\n",
              "      <td>JM Reitz - 2004 - books.google.com</td>\n",
              "      <td>What began in 1994 as a five-page handout, the...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Title  ...                                           Abstract\n",
              "0                   Information science: what is it?  ...  In seeking a new sense of identity, we ask, in...\n",
              "1  The foundations of information science. Part I...  ...  It is first argued that a niche for informatio...\n",
              "2  [BOOK][B] Foundations of library and informati...  ...  Much has happened since the last edition of th...\n",
              "3                   Geographical information science  ...  Research papers at conferences such as EGIS an...\n",
              "4  [BOOK][B] Dictionary for library and informati...  ...  What began in 1994 as a five-page handout, the...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P81xI3xpgAT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONu0Glf_5zNT"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X761gCGRpf84"
      },
      "source": [
        ""
      ]
    }
  ]
}